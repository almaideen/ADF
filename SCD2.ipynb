{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13982bb-4383-409b-9e4b-4cc317b39178",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768023596803}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# yesterday’s file (assuming you process next day)\n",
    "process_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y/%m/%d\")\n",
    "#process_date = datetime.today().strftime(\"%Y/%m/%d\") #--today's file\n",
    "\n",
    "path = f\"abfss://input@adlssource0001.dfs.core.windows.net/sftp/{process_date}/*.parquet\"\n",
    "\n",
    "\n",
    "raw_df = spark.read.format(\"parquet\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(path)\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9856356b-7055-4e3e-bf6a-1b2f8f79150c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a57f38-59d3-4b71-922e-2c18e32befe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_path = \"abfss://output@adlstarget0001.dfs.core.windows.net/curated_sftp/order_customers_delta\"\n",
    "\n",
    "(raw_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(target_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b6bfaa-ef09-4072-82ed-9584c8cc3789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS indiametastore.default.order_customers_delta\n",
    "USING DELTA\n",
    "LOCATION \"abfss://output@adlstarget0001.dfs.core.windows.net/curated_sftp/order_customers_delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfdf9b72-f0e3-4d6d-ac00-57d3c8152c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SCD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62be9367-9029-424f-b843-17d2d2d582d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, lit, expr\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# =========================================\n",
    "# CONFIG\n",
    "# =========================================\n",
    "# Base folder where ADF writes CSV→Parquet bulk\n",
    "sftp_base = \"abfss://input@adlssource0001.dfs.core.windows.net/sftp\"\n",
    "\n",
    "# Delta target for SCD2 table\n",
    "delta_path = \"abfss://output@adlstarget0001.dfs.core.windows.net/curated_sftp/order_customers_delta\"\n",
    "\n",
    "# Business key for SCD2\n",
    "key_cols = [\"customer_id\",\"customer_unique_id\"]\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 1: Find latest partition folder (YYYY/MM/DD)\n",
    "# =========================================\n",
    "years = dbutils.fs.ls(sftp_base + \"/\")\n",
    "year_names = [f.name.rstrip('/') for f in years if f.isDir()]\n",
    "if not year_names:\n",
    "    raise ValueError(\"No year folders found under csv_base.\")\n",
    "\n",
    "latest_year = max(year_names)\n",
    "\n",
    "months = dbutils.fs.ls(f\"{sftp_base}/{latest_year}/\")\n",
    "month_names = [f.name.rstrip('/') for f in months if f.isDir()]\n",
    "if not month_names:\n",
    "    raise ValueError(f\"No month folders found under year {latest_year}.\")\n",
    "\n",
    "latest_month = max(month_names)\n",
    "\n",
    "days = dbutils.fs.ls(f\"{sftp_base}/{latest_year}/{latest_month}/\")\n",
    "day_names = [f.name.rstrip('/') for f in days if f.isDir()]\n",
    "if not day_names:\n",
    "    raise ValueError(f\"No day folders found under {latest_year}/{latest_month}.\")\n",
    "\n",
    "latest_day = max(day_names)\n",
    "\n",
    "latest_path = f\"{sftp_base}/{latest_year}/{latest_month}/{latest_day}/*.parquet\"\n",
    "print(f\"Reading from: {latest_path}\")\n",
    "\n",
    "file_date = f\"{latest_year}-{latest_month}-{latest_day}\"\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 2: Read raw data and add SCD2/audit columns\n",
    "# =========================================\n",
    "raw_df = spark.read.parquet(latest_path)\n",
    "\n",
    "new_df = (raw_df\n",
    "          .withColumn(\"ingest_date\", current_date())\n",
    "          .withColumn(\"file_date\", lit(file_date))\n",
    "          .withColumn(\"updated_at\", current_timestamp())\n",
    "          .withColumn(\"customer_dim_id\", expr(\"uuid()\"))\n",
    "          .withColumn(\"is_active\", lit(1))\n",
    "          .withColumn(\"start_date\", current_date())\n",
    "          .withColumn(\"end_date\", lit(None).cast(\"date\")))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 3: Initial load or align schema\n",
    "# =========================================\n",
    "if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "    print(\"Delta table path does not exist yet. Performing initial load.\")\n",
    "    (new_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(delta_path))\n",
    "else:\n",
    "    print(\"Delta table exists. Aligning schema if needed, then performing SCD2 merge.\")\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "    existing_columns = [f.name for f in deltaTable.toDF().schema.fields]\n",
    "\n",
    "    df = deltaTable.toDF()\n",
    "\n",
    "    # Ensure SCD2 columns\n",
    "    if \"customer_dim_id\" not in existing_columns:\n",
    "        df = df.withColumn(\"customer_dim_id\", lit(None).cast(\"string\"))\n",
    "    if \"is_active\" not in existing_columns:\n",
    "        df = df.withColumn(\"is_active\", lit(1))\n",
    "    if \"start_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"start_date\", current_date())\n",
    "    if \"end_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"end_date\", lit(None).cast(\"date\"))\n",
    "\n",
    "    # Ensure audit columns\n",
    "    if \"file_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"file_date\", lit(None).cast(\"string\"))\n",
    "    if \"ingest_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"ingest_date\", lit(None).cast(\"date\"))\n",
    "    if \"updated_at\" not in existing_columns:\n",
    "        df = df.withColumn(\"updated_at\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "    # Overwrite with aligned schema\n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .save(delta_path))\n",
    "\n",
    "    # Reload as DeltaTable for merge\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # =========================================\n",
    "    # STEP 4: SCD2 merge\n",
    "    # =========================================\n",
    "    join_cond = \" AND \".join([f\"t.{c} = s.{c}\" for c in key_cols])\n",
    "\n",
    "    (deltaTable.alias(\"t\")\n",
    "        .merge(\n",
    "            new_df.alias(\"s\"),\n",
    "            join_cond\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"\"\"\n",
    "                t.is_active = 1 AND (\n",
    "                    t.customer_zip_code_prefix <> s.customer_zip_code_prefix OR\n",
    "                    t.customer_city <> s.customer_city OR\n",
    "                    t.customer_state <> s.customer_state\n",
    "                )\n",
    "            \"\"\",\n",
    "            set={\n",
    "                \"is_active\": \"0\",\n",
    "                \"end_date\": \"current_date()\"\n",
    "            }\n",
    "        )\n",
    "        .whenNotMatchedInsert(values={\n",
    "            \"customer_dim_id\": \"s.customer_dim_id\",\n",
    "            \"customer_id\": \"s.customer_id\",\n",
    "            \"customer_unique_id\": \"s.customer_unique_id\",\n",
    "            \"customer_zip_code_prefix\": \"s.customer_zip_code_prefix\",\n",
    "            \"customer_city\": \"s.customer_city\",\n",
    "            \"customer_state\": \"s.customer_state\",\n",
    "            \"ingest_date\": \"s.ingest_date\",\n",
    "            \"file_date\": \"s.file_date\",\n",
    "            \"updated_at\": \"s.updated_at\",\n",
    "            \"is_active\": \"1\",\n",
    "            \"start_date\": \"current_date()\",\n",
    "            \"end_date\": \"null\"\n",
    "        })\n",
    "        .execute())\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 5: Check results\n",
    "# =========================================\n",
    "print(\"Before Merge (raw_df):\")\n",
    "raw_df.show(10, truncate=False)\n",
    "\n",
    "print(\"After Merge (Delta SCD2 table):\")\n",
    "updated_df = spark.read.format(\"delta\").load(delta_path)\n",
    "updated_df.show(20, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4624923184185862,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
