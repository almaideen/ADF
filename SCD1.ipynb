{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19233d0b-88e4-40c0-aa74-ab39ecbca367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# yesterdayâ€™s file (assuming you process next day)\n",
    "process_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y/%m/%d\")\n",
    "#process_date = datetime.today().strftime(\"%Y/%m/%d\") #--today's file\n",
    "\n",
    "path = f\"abfss://input@adlssource0001.dfs.core.windows.net/csv/{process_date}/*.parquet\"\n",
    "\n",
    "\n",
    "raw_df = spark.read.format(\"parquet\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(path)\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f1e349-547e-4f4e-9c32-cf501d842719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_path = \"abfss://output@adlstarget0001.dfs.core.windows.net/curated_csv/order_payments_delta\"\n",
    "\n",
    "(raw_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(target_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fae23d-1eae-4edd-86bf-c46642e92749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS indiametastore.default.order_items_delta\n",
    "USING DELTA\n",
    "LOCATION \"abfss://output@adlstarget0001.dfs.core.windows.net/curated_csv/order_payments_delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d54ae37-567f-45c1-8df9-86429d723fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;\n",
    "SHOW SCHEMAS IN indiametastore;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795168c8-75d7-4060-93b6-702a47742dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"indiametastore.default.order_items_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "735e89ff-cb0f-466d-b501-eb7fd8c7cde0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SCD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6839c74f-5b45-4b3a-8a4a-69cab3eb1a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_date, lit, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "# Base folder where Snowflake parquet exports land\n",
    "csv_base = \"abfss://input@adlssource0001.dfs.core.windows.net/csv\"\n",
    "\n",
    "# Delta target folder on ADLS for SCD1 table\n",
    "delta_path = \"abfss://output@adlstarget0001.dfs.core.windows.net/curated_csv/order_payments_delta\"\n",
    "\n",
    "# Columns that define the business key (SCD1 merge keys)\n",
    "key_cols = [\"ORDER_ID\"]    # change if your keys are different\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: Find latest partition folder (YYYY/MM/DD)\n",
    "# ==============================\n",
    "\n",
    "# List years\n",
    "years = dbutils.fs.ls(csv_base + \"/\")\n",
    "year_names = [f.name.rstrip('/') for f in years if f.isDir()]\n",
    "if not year_names:\n",
    "    raise ValueError(\"No year folders found under snowflake_base.\")\n",
    "\n",
    "latest_year = max(year_names)\n",
    "\n",
    "# List months for that year\n",
    "months = dbutils.fs.ls(f\"{csv_base}/{latest_year}/\")\n",
    "month_names = [f.name.rstrip('/') for f in months if f.isDir()]\n",
    "if not month_names:\n",
    "    raise ValueError(f\"No month folders found under year {latest_year}.\")\n",
    "\n",
    "latest_month = max(month_names)\n",
    "\n",
    "# List days for that year/month\n",
    "days = dbutils.fs.ls(f\"{csv_base}/{latest_year}/{latest_month}/\")\n",
    "day_names = [f.name.rstrip('/') for f in days if f.isDir()]\n",
    "if not day_names:\n",
    "    raise ValueError(f\"No day folders found under {latest_year}/{latest_month}.\")\n",
    "\n",
    "latest_day = max(day_names)\n",
    "\n",
    "latest_path = f\"{csv_base}/{latest_year}/{latest_month}/{latest_day}/*.parquet\"\n",
    "print(f\"Reading from: {latest_path}\")\n",
    "\n",
    "file_date = f\"{latest_year}-{latest_month}-{latest_day}\"\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: Read raw data and add columns\n",
    "# ==============================\n",
    "raw_df = spark.read.parquet(latest_path)\n",
    "\n",
    "#spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "new_df = (raw_df\n",
    "          .withColumn(\"ingest_date\", current_date())   # load date\n",
    "          .withColumn(\"file_date\", lit(file_date)))    # folder date\n",
    "\n",
    "# ==============================\n",
    "# STEP 2.1: Deduplicate on key columns\n",
    "# ==============================\n",
    "window = Window.partitionBy(*key_cols).orderBy(new_df[\"ingest_date\"].desc())\n",
    "\n",
    "new_df = (new_df\n",
    "          .withColumn(\"rn\", row_number().over(window))\n",
    "          .filter(\"rn = 1\")\n",
    "          .drop(\"rn\"))\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: Merge into Delta (SCD1)\n",
    "# ==============================\n",
    "if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "    print(\"Delta table path does not exist yet. Performing initial load.\")\n",
    "    (new_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(delta_path))\n",
    "else:\n",
    "    print(\"Delta table exists. Performing SCD1 merge.\")\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # Build join condition string from key_cols\n",
    "    join_cond = \" AND \".join([f\"t.{c} = s.{c}\" for c in key_cols])\n",
    "\n",
    "    (deltaTable.alias(\"t\")\n",
    "        .merge(new_df.alias(\"s\"), join_cond)\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "# ==============================\n",
    "# STEP 4: Reload and quick check\n",
    "# ==============================\n",
    "updated_df = (spark.read\n",
    "              .format(\"delta\")\n",
    "              .option(\"mergeSchema\", \"true\")\n",
    "              .load(delta_path))\n",
    "\n",
    "print(\"Sample from source (raw_df):\")\n",
    "raw_df.show(10, truncate=False)\n",
    "\n",
    "print(\"Sample from SCD1 Delta (updated_df):\")\n",
    "updated_df.show(10, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4624923184185847,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
